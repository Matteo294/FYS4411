\begin{figure}[H]
    \begin{tikzpicture}
        
        \node (system) [draw, line width=0.3mm, text width=0.13\textwidth, minimum height=30pt, text centered] at (0,2) {\bfseries \texttt{System}};
        
        \node (functions) [draw, line width=0.3mm, text width=0.13\textwidth, minimum height=30pt, text centered] at (3,2) {\bfseries \texttt{Functions} };
        
        \node (random) [draw, line width=0.3mm, text width=0.13\textwidth, minimum height=30pt, text centered] at (-3,2) {\bfseries \texttt{Random \\ Generator} };
        
        \node (wavefunction) [draw, line width=0.3mm, text width=0.14\textwidth, minimum height=30pt, text centered] at (0,0) {\bfseries \texttt{Wavefunction}};
        \node (gauss) [draw, anchor=north, text width=0.13\textwidth, text centered] at (0, -1) {\texttt{Gaussian}};
        \node (asymmgauss) [draw, anchor=north, text width=0.13\textwidth, text centered] at (0, -2) { \texttt{Asymmetric \\ Gaussian}};
        
        \node (solver) [draw, line width=0.3mm, text width=0.13\textwidth, minimum height=30pt, text centered] at (3,0) {\bfseries \texttt{Solver}};
        \node (metropolis) [draw, anchor=north, text width=0.13\textwidth, text centered] at (3, -1) {\texttt{Metropolis}};
        \node (importance) [draw, anchor=north, text width=0.13\textwidth, text centered] at (3, -2) { \texttt{Importance \\ Sampling}};
        
        \node (hamiltonian) [draw, line width=0.3mm, text width=0.13\textwidth, minimum height=30pt, text centered] at (-3,0) {\bfseries \texttt{Hamiltonian}};
        \node (spherical) [draw, anchor=north, text width=0.13\textwidth, text centered] at (-3, -1) {\texttt{Spherical}};
        \node (elliptical) [draw, anchor=north, text width=0.13\textwidth, text centered] at (-3, -2) {\texttt{Elliptical}};
        
        \draw (system.south) -- (wavefunction.north);
        \draw (system.south) -- (hamiltonian.north);
        \draw (system.south) -- (solver.north);
        \draw (system.west) -- (random.east);
        \draw (system.east) -- (functions.west);
        
        \draw [dotted] (hamiltonian.south) -- (spherical.north);
        \draw [dotted] (spherical.south) -- (elliptical.north);
        \draw [dotted] (wavefunction.south) -- (gauss.north);
        \draw [dotted] (gauss.south) -- (asymmgauss.north);
        \draw [dotted] (solver.south) -- (metropolis.north);
        \draw [dotted] (metropolis.south) -- (importance.north);
    \end{tikzpicture}
    \caption{Classes diagram and hierarchy. Elliptical and spherical are Hamiltonian's subclasses, Gaussian and AsymmetricGaussian are Wavefunction's sublclasses, Metropolis and ImportanceSampling are Solver's sublasses.}
    \label{diag:classes_diagram}
\end{figure}

We chose C\texttt{++} as the main programming language to implement the aforementioned methods for this project. This allowed us to keep a good compromise between computational speed and abstraction. Some python algorithms were adopted for post-analysis operations and plots construction. The C\texttt{++} algorithm has been fully object oriented making the code modular, in the sense one can in principle reuse the code for other purposes by only replacing some pieces and keeping the general structure. All the implemented classes can be categorized as follows:
\begin{itemize}
    \item \texttt{System}: contains information on the system as a whole (includes for example the collection of particles) and provides a reference point for communication between the other classes;
    \item \texttt{Solvers}: provide different tools to perform the actual VMC run and evaluate the energy of the system for a chosen Hamiltonian and a selected wavefunction on different complexity levels, depending on the needs;
    \item \texttt{Wavefunctions}: the wavefunction describing the whole system is implemented here, together with the possibility of adjusting the parameters entering in its definition;
    \item \texttt{Hamiltonians}: this includes functions for the evaluation of the local energy;
    \item \texttt{Functions}: this includes some ad-hoc implemented functions that combine elements of the previously cited classes and perform specific tasks requested for the project;
    \item \texttt{RandomGenerator}: it contains functions for the generation of uniform and gaussian distributed random numbers.
\end{itemize}
The code has been documented via Doxygen and the documentation, together with the code itself, can be found at the GitHub link provided at the end of the report, hence hereby we discuss only some of the peculiar points of the implementation. The tests conducted to verify the solidity and the correctness of the results provided by the code will be described in the next section.

For the sake of clarity, the below section of pseudo-code describes the structure that we gave to our implementation. 


\begin{minted}[frame=lines, bgcolor=lemonchiffon]{cpp}
#include <defined_classes>

/* Select between parallelized and standard 
execution */
bool parallel;

// CREATE OBJECTS
System system(int dimension, int N_particles);

Hamiltonian hamiltonian(&system);
//spherical or elliptical

Wavefunction wavefunction(&systems);
// gaussian or antisymmetric gaussian

Solver solver(&system);
// metropolis or importance

Functions functions(&system);
RandomGenerator randomgenerator(&system);

// set the attributes of the system
system.setHamiltonian(&hamiltonian);
system.setWavefunction(&wavefunction);
system.setSolver(&solver);
system.setRandomGenerator(&randomgenerator);

// perform tasks
functions.task();
\end{minted}



\subsection{SINGLE-PARTICLE WAVEFUNCTION EVALUATION}
According to what previously discussed, for every VMC step we are in need to perform an acceptance test for the proposed move by evaluating a ratio of wavefunctions. Eq.\,\ref{wavefunctions} together with the fact that for each VMC step only one particle is moved, one infers that the acceptance test can be carried out by evaluating only those components in the wavefunction that include information on the moved particle. More in detail, in the non-interacting case if the wavefunction at the step $k$ of the simulation is
\begin{equation*}
    \Psi_T^{k} \equiv \Psi(\bm{R}(t_k)) = \prod_{i=1}^{N} g(\alpha, \bm{r}_i(t_k)) \equiv \prod_{i=1}^N g_i^{(k)}
\end{equation*}
then, if at the step $k+1$ we modify the position of the particle $n$, the wavefunction becomes
\begin{equation*}
    \Psi_T^{k+1} \ = \ g_n^{(k+1)} \, \prod_{\substack{i=1\\ i \neq n}} g_i^{k}
\end{equation*}
Thus the ratio between the wavefunctions appearing in Eq.\,\ref{acceptance_metropolis} and Eq.\,\ref{acceptance_importance} reduces to $\vert g_n^{(k+1)} \vert^2 / \vert g_n^{(k)} \vert^2$, allowing a reduction in the computational time. 


\subsection{RELATIVE POSITION BETWEEN PARTICLES IN THE INTERACTING CASE}
\label{sec:matrix_relative_dist}
The interacting case leads to way more complicated expressions for the wavefunction and the local energy; therefore, finding a clever way to reduce as much as possible the computational effort, is not so trivial as in the non-interacting case. However we noticed the repetitive demand for the same quantities, like the relative positions and relative distances between particles, in different expressions. Hence, we stored those values in proper matrices in order to avoid repetitive re-evaluations of those high time-demanding operations once that the position of the particles is set after a simulation step. The $ij$-th element of the distance matrix $D_{ij}$ is the distance between the particle $i$ and particle $j$: here one can note that this matrix is symmetrical, hence only one half of the off-diagonal values must be computed. In a similar way the $ij$-th element of the relative position matrix $R_{ij} = \bm{r}_i - \bm{r}_j$ is the relative position, in cartesian coordinates, of the particles $i$ with respect to to the particle $j$: this matrix is antisymmetrical, hence, also in this case, only half of the off-diagonal elements can be computed. The matrices are built and initialised at the beginning of the program, and after one particle is moved, only the corresponding rows and columns in the matrices are updated.



    
\subsection{CODE PARALLELIZATION}
The code has been parallelized via OpenMP. The strategy we adopted consisted in creating a copy of the system for every thread, each one with the same settings, and the selected number of steps for a single MC run was  equally split into the threads. Each parallel simulation was thus carried with a reduced number of steps and data generated from each thread was saved into different files, successively collected together in a python environment and the analysis was carried on the whole dataset. The  computational time improvement was analyzed on a 4-cores computer, observing a reduction factor in the simulation time which laid between $3.0$ and $3.5$.

A slightly different approach regards the gradient descent process, for which an independent research was launched on every thread, each of them providing us at the end with a different estimate of $\alpha_{GS}$ for the same configuration of the system. 
